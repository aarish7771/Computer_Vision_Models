{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7BQCRVRQlz9e55HPAz8gM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aarish7771/Computer_Vision_Models/blob/main/DCGANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Convoluted Generative Adversarial Networks (DCGANs)"
      ],
      "metadata": {
        "id": "aw0fgqKlDe_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing Libraries"
      ],
      "metadata": {
        "id": "_5kQoBUTDype"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LAqpHK63DUKI"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setting Hyperparameters"
      ],
      "metadata": {
        "id": "mKo9gCviE_c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batchSize = 64\n",
        "imageSize = 64"
      ],
      "metadata": {
        "id": "itlMByu3FJSL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating the Transformations"
      ],
      "metadata": {
        "id": "gTpM_W6uFSfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.Resize((imageSize, imageSize)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])"
      ],
      "metadata": {
        "id": "qa-lFWolFXhU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading the Dataset"
      ],
      "metadata": {
        "id": "3d1JqXcoFfz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dset.CIFAR10(root = './data', download = True, transform = transform)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKfTaqZUFmSQ",
        "outputId": "61012dd3-1ae0-4440-a5dd-65f2faf15ddb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining the weights_init function that takes as input a neural network m and that will initialize all its weights"
      ],
      "metadata": {
        "id": "aj19orhyJW7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)"
      ],
      "metadata": {
        "id": "dOEklzk3JEgq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the Generator"
      ],
      "metadata": {
        "id": "usqKW3_QJiaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class G(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(G, self).__init__()\n",
        "    self.main = nn.Sequential(\n",
        "        nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),\n",
        "        nn.Tanh()\n",
        "        )\n",
        "\n",
        "  def forward(self, input):\n",
        "    output = self.main(input)\n",
        "    return output"
      ],
      "metadata": {
        "id": "kDP1APy2MF-b"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the Generator"
      ],
      "metadata": {
        "id": "rqD7Prt0TLjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NetG = G()\n",
        "NetG.apply(weights_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwcHnfIGTOlZ",
        "outputId": "8feb3d6f-5695-4a0d-b7aa-db621c7abe3a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "G(\n",
              "  (main): Sequential(\n",
              "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (13): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the Discriminator"
      ],
      "metadata": {
        "id": "cpYxvxfUTXaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class D(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(D, self).__init__()\n",
        "    self.main = nn.Sequential(\n",
        "        nn.Conv2d(3, 64, 4, 2, 1, bias = False),\n",
        "        nn.LeakyReLU(0.2, inplace = True),\n",
        "        nn.Conv2d(64, 128, 4, 2, 1, bias = False),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.LeakyReLU(0.2, inplace = True),\n",
        "        nn.Conv2d(128, 256, 4, 2, 1, bias = False),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.LeakyReLU(0.2, inplace = True),\n",
        "        nn.Conv2d(256, 512, 4, 2, 1, bias = False),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.LeakyReLU(0.2, inplace = True),\n",
        "        nn.Conv2d(512, 1, 4, 1, 0, bias = False),\n",
        "        nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "  def forward(self, input):\n",
        "    output = self.main(input)\n",
        "    return output.view(-1)"
      ],
      "metadata": {
        "id": "FbFeNCU5g3F4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the Discriminator"
      ],
      "metadata": {
        "id": "Yd8Ykf_QkPR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NetD = D()\n",
        "NetD.apply(weights_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F23ZeFjmkSkG",
        "outputId": "27104478-78fe-40bc-f2ea-dc3b5b640caa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "D(\n",
              "  (main): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "    (12): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the DCGANs"
      ],
      "metadata": {
        "id": "CYhy8SSEke12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizerD = optim.Adam(NetD.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
        "optimizerG = optim.Adam(NetG.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
        "\n",
        "for epoch in range(25):\n",
        "  for i, data in enumerate(dataloader, 0):\n",
        "\n",
        "    # 1st Step: Updating the weights of the Neural Network of the Discriminator\n",
        "\n",
        "    NetD.zero_grad()\n",
        "\n",
        "    # Training the Discriminator with the real images of the Dataset\n",
        "    real, _ = data\n",
        "    input = Variable(real)\n",
        "    target = Variable(torch.ones(input.size()[0]))\n",
        "    output = NetD(input)\n",
        "    errD_real = criterion(output, target)\n",
        "\n",
        "    # Traininig the Discriminator with the fake images generated by the Generator\n",
        "    noise = Variable(torch.randn(input.size()[0], 100, 1, 1))\n",
        "    fake = NetG(noise)\n",
        "    target = Variable(torch.zeros(input.size()[0]))\n",
        "    output = NetD(fake.detach())\n",
        "    errD_fake = criterion(output, target)\n",
        "\n",
        "    # Back-propagating the total error\n",
        "    errD = errD_real + errD_fake\n",
        "    errD.backward()\n",
        "    optimizerD.step()\n",
        "\n",
        "    # 2nd Step: Updating the weights of the Neural Network of the Generator\n",
        "\n",
        "    NetG.zero_grad()\n",
        "    target = Variable(torch.ones(input.size()[0]))\n",
        "    output = NetD(fake)\n",
        "    errG = criterion(output, target)\n",
        "    errG.backward()\n",
        "    optimizerG.step()\n",
        "\n",
        "    # 3rd Step: Printing the losses and saving the Real and Generated Images of the Minibatch at every 100th step\n",
        "    print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 25, i, len(dataloader), errD, errG))\n",
        "    if i % 100 == 0:\n",
        "      vutils.save_image(real, '%s/real_samples.png' % \"./Results\", normalize = True)\n",
        "      fake = NetG(noise)\n",
        "      vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"./Results\", epoch), normalize = True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c05ynzrg-h5-",
        "outputId": "32e5940d-64c4-4493-8f2b-45193a61001d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0/25][0/782] Loss_D: 1.7160 Loss_G: 7.2397\n",
            "[0/25][1/782] Loss_D: 1.0190 Loss_G: 4.8311\n",
            "[0/25][2/782] Loss_D: 1.1474 Loss_G: 5.0672\n",
            "[0/25][3/782] Loss_D: 1.2330 Loss_G: 6.3412\n",
            "[0/25][4/782] Loss_D: 0.4788 Loss_G: 6.7453\n",
            "[0/25][5/782] Loss_D: 0.5957 Loss_G: 6.4565\n",
            "[0/25][6/782] Loss_D: 0.9307 Loss_G: 7.7577\n",
            "[0/25][7/782] Loss_D: 0.8932 Loss_G: 8.0372\n",
            "[0/25][8/782] Loss_D: 0.7526 Loss_G: 7.5883\n",
            "[0/25][9/782] Loss_D: 0.6696 Loss_G: 9.7054\n",
            "[0/25][10/782] Loss_D: 0.4784 Loss_G: 7.2766\n",
            "[0/25][11/782] Loss_D: 1.1149 Loss_G: 11.7501\n",
            "[0/25][12/782] Loss_D: 0.7721 Loss_G: 9.0866\n",
            "[0/25][13/782] Loss_D: 0.6800 Loss_G: 8.9936\n",
            "[0/25][14/782] Loss_D: 0.7050 Loss_G: 12.5333\n",
            "[0/25][15/782] Loss_D: 0.2976 Loss_G: 9.9467\n",
            "[0/25][16/782] Loss_D: 0.5430 Loss_G: 11.6576\n",
            "[0/25][17/782] Loss_D: 0.3139 Loss_G: 8.8241\n",
            "[0/25][18/782] Loss_D: 0.6820 Loss_G: 11.6179\n",
            "[0/25][19/782] Loss_D: 0.1414 Loss_G: 9.4662\n",
            "[0/25][20/782] Loss_D: 0.4259 Loss_G: 11.9910\n",
            "[0/25][21/782] Loss_D: 0.1104 Loss_G: 9.8044\n",
            "[0/25][22/782] Loss_D: 0.4235 Loss_G: 12.0944\n",
            "[0/25][23/782] Loss_D: 0.5938 Loss_G: 8.4175\n",
            "[0/25][24/782] Loss_D: 0.7494 Loss_G: 16.7667\n",
            "[0/25][25/782] Loss_D: 0.1494 Loss_G: 16.0971\n",
            "[0/25][26/782] Loss_D: 0.1456 Loss_G: 9.8500\n",
            "[0/25][27/782] Loss_D: 0.7156 Loss_G: 17.2131\n",
            "[0/25][28/782] Loss_D: 0.1678 Loss_G: 16.3290\n",
            "[0/25][29/782] Loss_D: 0.2689 Loss_G: 9.7307\n",
            "[0/25][30/782] Loss_D: 0.9406 Loss_G: 19.3303\n",
            "[0/25][31/782] Loss_D: 0.3420 Loss_G: 19.6090\n",
            "[0/25][32/782] Loss_D: 0.1153 Loss_G: 14.3809\n",
            "[0/25][33/782] Loss_D: 0.0420 Loss_G: 6.3491\n",
            "[0/25][34/782] Loss_D: 2.2353 Loss_G: 23.4992\n",
            "[0/25][35/782] Loss_D: 0.2674 Loss_G: 26.7855\n",
            "[0/25][36/782] Loss_D: 0.2181 Loss_G: 25.1359\n",
            "[0/25][37/782] Loss_D: 0.3088 Loss_G: 19.3690\n",
            "[0/25][38/782] Loss_D: 0.0126 Loss_G: 10.7442\n",
            "[0/25][39/782] Loss_D: 0.2176 Loss_G: 9.4360\n",
            "[0/25][40/782] Loss_D: 0.3025 Loss_G: 15.5497\n",
            "[0/25][41/782] Loss_D: 0.0843 Loss_G: 14.9565\n",
            "[0/25][42/782] Loss_D: 0.0598 Loss_G: 9.2515\n",
            "[0/25][43/782] Loss_D: 0.7543 Loss_G: 21.3601\n",
            "[0/25][44/782] Loss_D: 0.1240 Loss_G: 23.6931\n",
            "[0/25][45/782] Loss_D: 0.2508 Loss_G: 20.2617\n",
            "[0/25][46/782] Loss_D: 0.2878 Loss_G: 12.7390\n",
            "[0/25][47/782] Loss_D: 0.0914 Loss_G: 5.1217\n",
            "[0/25][48/782] Loss_D: 3.2497 Loss_G: 26.9560\n",
            "[0/25][49/782] Loss_D: 0.2002 Loss_G: 32.3424\n",
            "[0/25][50/782] Loss_D: 0.1421 Loss_G: 34.1829\n",
            "[0/25][51/782] Loss_D: 0.1698 Loss_G: 34.8435\n",
            "[0/25][52/782] Loss_D: 0.0731 Loss_G: 34.6380\n",
            "[0/25][53/782] Loss_D: 0.0517 Loss_G: 34.7046\n",
            "[0/25][54/782] Loss_D: 0.0103 Loss_G: 34.6163\n",
            "[0/25][55/782] Loss_D: 0.0187 Loss_G: 34.3341\n",
            "[0/25][56/782] Loss_D: 0.0117 Loss_G: 34.3864\n",
            "[0/25][57/782] Loss_D: 0.0112 Loss_G: 34.1625\n",
            "[0/25][58/782] Loss_D: 0.0106 Loss_G: 33.8560\n",
            "[0/25][59/782] Loss_D: 0.0062 Loss_G: 33.5878\n",
            "[0/25][60/782] Loss_D: 0.0035 Loss_G: 33.7812\n",
            "[0/25][61/782] Loss_D: 0.0155 Loss_G: 33.3430\n",
            "[0/25][62/782] Loss_D: 0.0069 Loss_G: 32.9261\n",
            "[0/25][63/782] Loss_D: 0.0026 Loss_G: 32.5675\n",
            "[0/25][64/782] Loss_D: 0.0040 Loss_G: 32.0285\n",
            "[0/25][65/782] Loss_D: 0.0055 Loss_G: 31.4460\n",
            "[0/25][66/782] Loss_D: 0.0024 Loss_G: 30.3760\n",
            "[0/25][67/782] Loss_D: 0.0044 Loss_G: 28.5520\n",
            "[0/25][68/782] Loss_D: 0.0064 Loss_G: 24.2664\n",
            "[0/25][69/782] Loss_D: 0.0027 Loss_G: 16.4600\n",
            "[0/25][70/782] Loss_D: 0.0050 Loss_G: 7.5584\n",
            "[0/25][71/782] Loss_D: 0.6299 Loss_G: 19.9296\n",
            "[0/25][72/782] Loss_D: 0.0638 Loss_G: 22.3650\n",
            "[0/25][73/782] Loss_D: 0.1206 Loss_G: 19.5758\n",
            "[0/25][74/782] Loss_D: 0.0443 Loss_G: 14.2178\n",
            "[0/25][75/782] Loss_D: 0.0258 Loss_G: 8.4432\n",
            "[0/25][76/782] Loss_D: 0.1269 Loss_G: 6.8572\n",
            "[0/25][77/782] Loss_D: 0.3492 Loss_G: 13.5199\n",
            "[0/25][78/782] Loss_D: 0.1861 Loss_G: 12.9079\n",
            "[0/25][79/782] Loss_D: 0.1645 Loss_G: 8.9454\n",
            "[0/25][80/782] Loss_D: 0.0661 Loss_G: 5.6528\n",
            "[0/25][81/782] Loss_D: 1.2970 Loss_G: 20.2897\n",
            "[0/25][82/782] Loss_D: 0.3148 Loss_G: 22.7206\n",
            "[0/25][83/782] Loss_D: 0.6135 Loss_G: 18.7860\n",
            "[0/25][84/782] Loss_D: 0.1211 Loss_G: 13.5107\n",
            "[0/25][85/782] Loss_D: 0.0433 Loss_G: 6.9067\n",
            "[0/25][86/782] Loss_D: 0.4332 Loss_G: 9.6123\n",
            "[0/25][87/782] Loss_D: 0.0712 Loss_G: 9.6099\n",
            "[0/25][88/782] Loss_D: 0.1071 Loss_G: 8.1118\n",
            "[0/25][89/782] Loss_D: 0.0673 Loss_G: 5.2288\n",
            "[0/25][90/782] Loss_D: 0.3237 Loss_G: 7.8041\n",
            "[0/25][91/782] Loss_D: 0.1725 Loss_G: 7.1843\n",
            "[0/25][92/782] Loss_D: 0.2063 Loss_G: 6.4812\n",
            "[0/25][93/782] Loss_D: 0.2857 Loss_G: 7.0038\n",
            "[0/25][94/782] Loss_D: 0.4675 Loss_G: 9.4646\n",
            "[0/25][95/782] Loss_D: 0.3174 Loss_G: 5.7266\n",
            "[0/25][96/782] Loss_D: 0.3898 Loss_G: 10.0214\n",
            "[0/25][97/782] Loss_D: 0.2180 Loss_G: 8.5919\n",
            "[0/25][98/782] Loss_D: 0.1346 Loss_G: 5.1299\n",
            "[0/25][99/782] Loss_D: 0.7543 Loss_G: 12.6421\n",
            "[0/25][100/782] Loss_D: 2.6670 Loss_G: 7.3061\n",
            "[0/25][101/782] Loss_D: 0.3051 Loss_G: 2.7740\n",
            "[0/25][102/782] Loss_D: 1.6608 Loss_G: 9.5751\n",
            "[0/25][103/782] Loss_D: 1.2732 Loss_G: 7.9730\n",
            "[0/25][104/782] Loss_D: 0.4728 Loss_G: 4.9805\n",
            "[0/25][105/782] Loss_D: 0.4916 Loss_G: 5.0730\n",
            "[0/25][106/782] Loss_D: 0.2207 Loss_G: 5.5959\n",
            "[0/25][107/782] Loss_D: 0.3077 Loss_G: 4.3683\n",
            "[0/25][108/782] Loss_D: 0.3234 Loss_G: 5.4810\n",
            "[0/25][109/782] Loss_D: 0.3512 Loss_G: 4.9447\n",
            "[0/25][110/782] Loss_D: 0.5148 Loss_G: 5.4411\n",
            "[0/25][111/782] Loss_D: 0.4743 Loss_G: 4.5051\n",
            "[0/25][112/782] Loss_D: 0.5240 Loss_G: 7.3998\n",
            "[0/25][113/782] Loss_D: 0.3535 Loss_G: 5.0662\n",
            "[0/25][114/782] Loss_D: 0.3635 Loss_G: 6.1411\n",
            "[0/25][115/782] Loss_D: 0.2944 Loss_G: 7.0461\n",
            "[0/25][116/782] Loss_D: 0.5763 Loss_G: 2.2015\n",
            "[0/25][117/782] Loss_D: 1.7718 Loss_G: 13.4812\n",
            "[0/25][118/782] Loss_D: 3.7362 Loss_G: 9.5483\n",
            "[0/25][119/782] Loss_D: 0.5058 Loss_G: 4.2385\n",
            "[0/25][120/782] Loss_D: 1.2297 Loss_G: 6.6857\n",
            "[0/25][121/782] Loss_D: 1.1880 Loss_G: 5.8518\n",
            "[0/25][122/782] Loss_D: 0.5259 Loss_G: 3.5044\n",
            "[0/25][123/782] Loss_D: 0.7117 Loss_G: 5.5047\n",
            "[0/25][124/782] Loss_D: 0.4819 Loss_G: 4.6954\n",
            "[0/25][125/782] Loss_D: 0.2743 Loss_G: 4.9261\n",
            "[0/25][126/782] Loss_D: 0.1266 Loss_G: 5.3493\n",
            "[0/25][127/782] Loss_D: 0.3437 Loss_G: 3.5746\n",
            "[0/25][128/782] Loss_D: 0.4363 Loss_G: 4.6734\n",
            "[0/25][129/782] Loss_D: 0.3046 Loss_G: 5.4339\n",
            "[0/25][130/782] Loss_D: 0.2763 Loss_G: 4.1581\n",
            "[0/25][131/782] Loss_D: 0.6811 Loss_G: 6.3764\n",
            "[0/25][132/782] Loss_D: 0.3676 Loss_G: 4.3420\n",
            "[0/25][133/782] Loss_D: 0.3762 Loss_G: 6.1789\n",
            "[0/25][134/782] Loss_D: 0.2544 Loss_G: 5.3173\n",
            "[0/25][135/782] Loss_D: 0.4298 Loss_G: 3.6747\n",
            "[0/25][136/782] Loss_D: 0.5549 Loss_G: 8.1665\n",
            "[0/25][137/782] Loss_D: 0.3077 Loss_G: 5.6252\n",
            "[0/25][138/782] Loss_D: 0.3605 Loss_G: 3.0364\n",
            "[0/25][139/782] Loss_D: 0.9318 Loss_G: 10.2411\n",
            "[0/25][140/782] Loss_D: 3.1652 Loss_G: 4.7428\n",
            "[0/25][141/782] Loss_D: 0.8119 Loss_G: 2.9190\n",
            "[0/25][142/782] Loss_D: 1.7419 Loss_G: 9.3145\n",
            "[0/25][143/782] Loss_D: 1.3288 Loss_G: 5.0495\n",
            "[0/25][144/782] Loss_D: 0.6700 Loss_G: 4.8415\n",
            "[0/25][145/782] Loss_D: 0.4913 Loss_G: 5.9204\n",
            "[0/25][146/782] Loss_D: 0.3205 Loss_G: 4.6560\n",
            "[0/25][147/782] Loss_D: 0.4092 Loss_G: 3.2577\n",
            "[0/25][148/782] Loss_D: 0.6257 Loss_G: 6.7397\n",
            "[0/25][149/782] Loss_D: 0.8007 Loss_G: 2.8718\n",
            "[0/25][150/782] Loss_D: 0.6816 Loss_G: 6.6553\n",
            "[0/25][151/782] Loss_D: 0.6245 Loss_G: 3.7669\n",
            "[0/25][152/782] Loss_D: 0.9125 Loss_G: 8.1293\n",
            "[0/25][153/782] Loss_D: 1.3516 Loss_G: 5.0598\n",
            "[0/25][154/782] Loss_D: 0.3551 Loss_G: 3.7575\n",
            "[0/25][155/782] Loss_D: 0.7671 Loss_G: 6.4173\n",
            "[0/25][156/782] Loss_D: 0.6229 Loss_G: 4.4637\n",
            "[0/25][157/782] Loss_D: 0.3608 Loss_G: 4.0674\n",
            "[0/25][158/782] Loss_D: 0.6030 Loss_G: 6.4616\n",
            "[0/25][159/782] Loss_D: 0.5396 Loss_G: 3.9079\n",
            "[0/25][160/782] Loss_D: 0.6102 Loss_G: 6.6466\n",
            "[0/25][161/782] Loss_D: 0.6228 Loss_G: 4.6062\n",
            "[0/25][162/782] Loss_D: 0.3958 Loss_G: 5.6928\n",
            "[0/25][163/782] Loss_D: 0.4050 Loss_G: 4.0105\n",
            "[0/25][164/782] Loss_D: 0.6851 Loss_G: 9.7926\n",
            "[0/25][165/782] Loss_D: 1.4047 Loss_G: 5.1828\n",
            "[0/25][166/782] Loss_D: 0.4328 Loss_G: 5.0936\n",
            "[0/25][167/782] Loss_D: 0.3701 Loss_G: 6.4644\n",
            "[0/25][168/782] Loss_D: 0.6254 Loss_G: 3.2352\n",
            "[0/25][169/782] Loss_D: 1.2488 Loss_G: 9.9236\n",
            "[0/25][170/782] Loss_D: 1.4051 Loss_G: 6.6700\n",
            "[0/25][171/782] Loss_D: 0.2753 Loss_G: 3.8731\n",
            "[0/25][172/782] Loss_D: 0.4304 Loss_G: 5.9916\n",
            "[0/25][173/782] Loss_D: 0.1861 Loss_G: 6.0686\n",
            "[0/25][174/782] Loss_D: 0.2865 Loss_G: 5.0963\n",
            "[0/25][175/782] Loss_D: 0.3424 Loss_G: 5.5119\n",
            "[0/25][176/782] Loss_D: 0.3354 Loss_G: 5.8781\n",
            "[0/25][177/782] Loss_D: 0.2678 Loss_G: 5.2993\n",
            "[0/25][178/782] Loss_D: 0.3307 Loss_G: 5.3593\n",
            "[0/25][179/782] Loss_D: 0.3320 Loss_G: 5.0665\n",
            "[0/25][180/782] Loss_D: 0.3230 Loss_G: 7.0006\n",
            "[0/25][181/782] Loss_D: 0.2468 Loss_G: 5.4659\n",
            "[0/25][182/782] Loss_D: 0.2282 Loss_G: 5.5709\n",
            "[0/25][183/782] Loss_D: 0.2282 Loss_G: 7.1237\n",
            "[0/25][184/782] Loss_D: 0.1852 Loss_G: 5.8358\n",
            "[0/25][185/782] Loss_D: 0.3038 Loss_G: 6.4208\n",
            "[0/25][186/782] Loss_D: 0.2353 Loss_G: 6.8659\n",
            "[0/25][187/782] Loss_D: 0.2448 Loss_G: 6.1345\n",
            "[0/25][188/782] Loss_D: 0.3422 Loss_G: 8.0838\n",
            "[0/25][189/782] Loss_D: 0.2123 Loss_G: 6.3635\n",
            "[0/25][190/782] Loss_D: 0.2592 Loss_G: 8.6115\n",
            "[0/25][191/782] Loss_D: 0.1853 Loss_G: 6.4628\n",
            "[0/25][192/782] Loss_D: 0.4203 Loss_G: 11.1619\n",
            "[0/25][193/782] Loss_D: 0.5323 Loss_G: 5.9364\n",
            "[0/25][194/782] Loss_D: 0.3016 Loss_G: 9.5741\n",
            "[0/25][195/782] Loss_D: 0.1526 Loss_G: 7.5969\n",
            "[0/25][196/782] Loss_D: 0.1281 Loss_G: 5.5695\n",
            "[0/25][197/782] Loss_D: 0.2950 Loss_G: 9.9426\n",
            "[0/25][198/782] Loss_D: 0.2754 Loss_G: 7.2741\n",
            "[0/25][199/782] Loss_D: 0.1325 Loss_G: 5.4181\n",
            "[0/25][200/782] Loss_D: 0.4929 Loss_G: 12.3541\n",
            "[0/25][201/782] Loss_D: 0.8667 Loss_G: 6.0099\n",
            "[0/25][202/782] Loss_D: 0.4600 Loss_G: 12.8318\n",
            "[0/25][203/782] Loss_D: 0.3272 Loss_G: 10.2510\n",
            "[0/25][204/782] Loss_D: 0.0801 Loss_G: 6.8609\n",
            "[0/25][205/782] Loss_D: 0.1561 Loss_G: 5.3568\n",
            "[0/25][206/782] Loss_D: 0.3663 Loss_G: 10.4728\n",
            "[0/25][207/782] Loss_D: 0.1529 Loss_G: 9.7310\n",
            "[0/25][208/782] Loss_D: 0.2055 Loss_G: 5.7786\n",
            "[0/25][209/782] Loss_D: 0.1203 Loss_G: 4.5926\n",
            "[0/25][210/782] Loss_D: 0.5051 Loss_G: 10.3433\n",
            "[0/25][211/782] Loss_D: 0.3867 Loss_G: 8.7182\n",
            "[0/25][212/782] Loss_D: 0.0477 Loss_G: 5.9725\n",
            "[0/25][213/782] Loss_D: 0.2576 Loss_G: 8.2249\n",
            "[0/25][214/782] Loss_D: 0.1290 Loss_G: 7.0982\n",
            "[0/25][215/782] Loss_D: 0.0998 Loss_G: 5.9879\n",
            "[0/25][216/782] Loss_D: 0.2533 Loss_G: 6.4484\n",
            "[0/25][217/782] Loss_D: 0.2304 Loss_G: 5.5574\n",
            "[0/25][218/782] Loss_D: 0.1004 Loss_G: 6.0464\n",
            "[0/25][219/782] Loss_D: 0.1528 Loss_G: 6.4316\n",
            "[0/25][220/782] Loss_D: 0.0898 Loss_G: 5.8433\n",
            "[0/25][221/782] Loss_D: 0.1351 Loss_G: 6.3702\n",
            "[0/25][222/782] Loss_D: 0.1131 Loss_G: 6.1753\n",
            "[0/25][223/782] Loss_D: 0.1507 Loss_G: 6.4090\n",
            "[0/25][224/782] Loss_D: 0.1039 Loss_G: 6.2429\n",
            "[0/25][225/782] Loss_D: 0.0821 Loss_G: 6.3022\n",
            "[0/25][226/782] Loss_D: 0.0849 Loss_G: 6.6965\n",
            "[0/25][227/782] Loss_D: 0.1281 Loss_G: 6.9649\n",
            "[0/25][228/782] Loss_D: 0.1086 Loss_G: 6.8853\n",
            "[0/25][229/782] Loss_D: 0.2160 Loss_G: 8.6249\n",
            "[0/25][230/782] Loss_D: 0.1299 Loss_G: 6.9390\n",
            "[0/25][231/782] Loss_D: 0.2312 Loss_G: 14.8711\n",
            "[0/25][232/782] Loss_D: 0.3005 Loss_G: 10.8533\n",
            "[0/25][233/782] Loss_D: 0.0897 Loss_G: 7.9337\n",
            "[0/25][234/782] Loss_D: 1.1865 Loss_G: 27.3916\n",
            "[0/25][235/782] Loss_D: 5.8707 Loss_G: 22.6392\n",
            "[0/25][236/782] Loss_D: 1.1579 Loss_G: 13.8931\n",
            "[0/25][237/782] Loss_D: 0.1331 Loss_G: 4.4185\n",
            "[0/25][238/782] Loss_D: 3.7292 Loss_G: 19.4199\n",
            "[0/25][239/782] Loss_D: 1.0038 Loss_G: 17.0766\n",
            "[0/25][240/782] Loss_D: 0.0985 Loss_G: 10.2859\n",
            "[0/25][241/782] Loss_D: 0.5137 Loss_G: 6.2597\n",
            "[0/25][242/782] Loss_D: 1.4499 Loss_G: 11.3539\n",
            "[0/25][243/782] Loss_D: 3.1671 Loss_G: 5.2725\n",
            "[0/25][244/782] Loss_D: 0.3506 Loss_G: 3.2052\n",
            "[0/25][245/782] Loss_D: 0.5574 Loss_G: 6.4192\n",
            "[0/25][246/782] Loss_D: 0.1743 Loss_G: 6.4688\n",
            "[0/25][247/782] Loss_D: 0.1015 Loss_G: 5.3972\n",
            "[0/25][248/782] Loss_D: 0.0933 Loss_G: 4.7176\n",
            "[0/25][249/782] Loss_D: 0.2429 Loss_G: 5.1371\n",
            "[0/25][250/782] Loss_D: 0.1591 Loss_G: 5.5023\n",
            "[0/25][251/782] Loss_D: 0.1298 Loss_G: 5.3923\n",
            "[0/25][252/782] Loss_D: 0.2737 Loss_G: 6.4024\n",
            "[0/25][253/782] Loss_D: 0.3309 Loss_G: 4.4014\n",
            "[0/25][254/782] Loss_D: 0.4979 Loss_G: 10.1845\n",
            "[0/25][255/782] Loss_D: 0.8123 Loss_G: 6.2746\n",
            "[0/25][256/782] Loss_D: 0.2205 Loss_G: 4.8620\n",
            "[0/25][257/782] Loss_D: 0.3066 Loss_G: 9.0768\n",
            "[0/25][258/782] Loss_D: 0.1785 Loss_G: 8.2808\n",
            "[0/25][259/782] Loss_D: 0.1659 Loss_G: 5.1556\n",
            "[0/25][260/782] Loss_D: 0.1976 Loss_G: 4.8034\n",
            "[0/25][261/782] Loss_D: 0.3650 Loss_G: 7.0826\n",
            "[0/25][262/782] Loss_D: 0.2350 Loss_G: 6.1022\n",
            "[0/25][263/782] Loss_D: 0.2046 Loss_G: 4.3010\n",
            "[0/25][264/782] Loss_D: 0.5737 Loss_G: 8.2221\n",
            "[0/25][265/782] Loss_D: 0.5064 Loss_G: 5.6097\n",
            "[0/25][266/782] Loss_D: 0.1063 Loss_G: 5.0504\n",
            "[0/25][267/782] Loss_D: 0.1609 Loss_G: 7.3490\n",
            "[0/25][268/782] Loss_D: 0.0929 Loss_G: 6.5468\n",
            "[0/25][269/782] Loss_D: 0.0652 Loss_G: 5.8516\n",
            "[0/25][270/782] Loss_D: 0.1582 Loss_G: 4.3323\n",
            "[0/25][271/782] Loss_D: 0.3263 Loss_G: 8.4826\n",
            "[0/25][272/782] Loss_D: 0.1892 Loss_G: 8.1066\n",
            "[0/25][273/782] Loss_D: 0.2019 Loss_G: 4.8568\n",
            "[0/25][274/782] Loss_D: 0.2429 Loss_G: 6.9237\n",
            "[0/25][275/782] Loss_D: 0.0880 Loss_G: 6.5517\n",
            "[0/25][276/782] Loss_D: 0.0975 Loss_G: 5.5721\n",
            "[0/25][277/782] Loss_D: 0.1928 Loss_G: 6.6246\n",
            "[0/25][278/782] Loss_D: 0.2068 Loss_G: 5.3045\n",
            "[0/25][279/782] Loss_D: 0.2145 Loss_G: 6.8859\n",
            "[0/25][280/782] Loss_D: 0.1913 Loss_G: 6.6025\n",
            "[0/25][281/782] Loss_D: 0.3113 Loss_G: 6.4285\n",
            "[0/25][282/782] Loss_D: 0.2286 Loss_G: 6.6282\n",
            "[0/25][283/782] Loss_D: 0.1725 Loss_G: 7.0658\n",
            "[0/25][284/782] Loss_D: 0.1981 Loss_G: 6.1417\n",
            "[0/25][285/782] Loss_D: 0.1837 Loss_G: 5.0233\n",
            "[0/25][286/782] Loss_D: 0.2279 Loss_G: 7.5422\n",
            "[0/25][287/782] Loss_D: 0.3484 Loss_G: 5.4367\n",
            "[0/25][288/782] Loss_D: 0.1701 Loss_G: 5.2839\n",
            "[0/25][289/782] Loss_D: 0.1492 Loss_G: 7.0580\n",
            "[0/25][290/782] Loss_D: 0.0503 Loss_G: 7.0830\n",
            "[0/25][291/782] Loss_D: 0.0805 Loss_G: 6.3692\n",
            "[0/25][292/782] Loss_D: 0.0879 Loss_G: 5.6257\n",
            "[0/25][293/782] Loss_D: 0.2022 Loss_G: 8.7488\n",
            "[0/25][294/782] Loss_D: 0.1753 Loss_G: 7.1579\n",
            "[0/25][295/782] Loss_D: 0.0827 Loss_G: 5.8256\n",
            "[0/25][296/782] Loss_D: 0.1913 Loss_G: 7.6791\n",
            "[0/25][297/782] Loss_D: 0.2640 Loss_G: 6.4386\n",
            "[0/25][298/782] Loss_D: 0.1205 Loss_G: 6.2461\n",
            "[0/25][299/782] Loss_D: 0.1211 Loss_G: 6.7501\n",
            "[0/25][300/782] Loss_D: 0.1013 Loss_G: 7.4298\n",
            "[0/25][301/782] Loss_D: 0.5837 Loss_G: 2.7879\n",
            "[0/25][302/782] Loss_D: 1.6459 Loss_G: 21.8562\n",
            "[0/25][303/782] Loss_D: 2.9067 Loss_G: 20.0106\n",
            "[0/25][304/782] Loss_D: 1.1574 Loss_G: 13.0701\n",
            "[0/25][305/782] Loss_D: 0.1046 Loss_G: 6.6228\n",
            "[0/25][306/782] Loss_D: 1.5824 Loss_G: 12.6032\n",
            "[0/25][307/782] Loss_D: 0.3559 Loss_G: 12.8024\n",
            "[0/25][308/782] Loss_D: 0.4808 Loss_G: 7.6714\n",
            "[0/25][309/782] Loss_D: 0.4499 Loss_G: 6.1933\n",
            "[0/25][310/782] Loss_D: 0.4797 Loss_G: 8.6053\n",
            "[0/25][311/782] Loss_D: 0.1547 Loss_G: 8.1335\n",
            "[0/25][312/782] Loss_D: 0.2736 Loss_G: 5.3856\n",
            "[0/25][313/782] Loss_D: 0.3875 Loss_G: 7.3673\n",
            "[0/25][314/782] Loss_D: 0.2043 Loss_G: 6.4295\n",
            "[0/25][315/782] Loss_D: 0.3370 Loss_G: 7.7538\n",
            "[0/25][316/782] Loss_D: 0.2911 Loss_G: 6.1905\n",
            "[0/25][317/782] Loss_D: 0.3335 Loss_G: 6.8113\n",
            "[0/25][318/782] Loss_D: 0.7451 Loss_G: 1.7238\n",
            "[0/25][319/782] Loss_D: 1.9317 Loss_G: 17.2159\n",
            "[0/25][320/782] Loss_D: 4.7531 Loss_G: 13.9435\n",
            "[0/25][321/782] Loss_D: 1.1099 Loss_G: 9.5173\n",
            "[0/25][322/782] Loss_D: 0.2106 Loss_G: 5.3383\n",
            "[0/25][323/782] Loss_D: 0.6216 Loss_G: 7.0443\n",
            "[0/25][324/782] Loss_D: 0.7365 Loss_G: 7.3049\n",
            "[0/25][325/782] Loss_D: 0.5060 Loss_G: 5.8203\n",
            "[0/25][326/782] Loss_D: 0.4802 Loss_G: 7.5914\n",
            "[0/25][327/782] Loss_D: 0.3588 Loss_G: 5.9021\n",
            "[0/25][328/782] Loss_D: 0.5140 Loss_G: 4.4111\n",
            "[0/25][329/782] Loss_D: 0.6536 Loss_G: 7.4067\n",
            "[0/25][330/782] Loss_D: 0.1647 Loss_G: 6.7535\n",
            "[0/25][331/782] Loss_D: 0.1696 Loss_G: 5.1794\n",
            "[0/25][332/782] Loss_D: 0.2286 Loss_G: 5.3230\n",
            "[0/25][333/782] Loss_D: 0.2612 Loss_G: 6.3383\n",
            "[0/25][334/782] Loss_D: 0.1648 Loss_G: 5.7500\n",
            "[0/25][335/782] Loss_D: 0.1532 Loss_G: 4.8955\n",
            "[0/25][336/782] Loss_D: 0.2311 Loss_G: 5.1375\n",
            "[0/25][337/782] Loss_D: 0.1755 Loss_G: 5.8089\n",
            "[0/25][338/782] Loss_D: 0.2032 Loss_G: 4.5722\n",
            "[0/25][339/782] Loss_D: 0.2102 Loss_G: 5.3037\n",
            "[0/25][340/782] Loss_D: 0.1180 Loss_G: 5.1965\n",
            "[0/25][341/782] Loss_D: 0.4906 Loss_G: 3.5182\n",
            "[0/25][342/782] Loss_D: 0.4268 Loss_G: 6.9497\n",
            "[0/25][343/782] Loss_D: 0.1455 Loss_G: 7.0137\n",
            "[0/25][344/782] Loss_D: 0.1020 Loss_G: 5.5314\n",
            "[0/25][345/782] Loss_D: 0.1509 Loss_G: 4.2838\n",
            "[0/25][346/782] Loss_D: 0.3164 Loss_G: 5.5527\n",
            "[0/25][347/782] Loss_D: 0.1175 Loss_G: 5.7054\n",
            "[0/25][348/782] Loss_D: 0.1328 Loss_G: 4.5808\n",
            "[0/25][349/782] Loss_D: 0.1953 Loss_G: 4.6800\n",
            "[0/25][350/782] Loss_D: 0.2106 Loss_G: 5.7905\n",
            "[0/25][351/782] Loss_D: 0.1893 Loss_G: 4.9375\n",
            "[0/25][352/782] Loss_D: 0.1536 Loss_G: 5.1141\n",
            "[0/25][353/782] Loss_D: 0.1700 Loss_G: 5.4699\n",
            "[0/25][354/782] Loss_D: 0.2224 Loss_G: 5.1097\n",
            "[0/25][355/782] Loss_D: 0.1741 Loss_G: 5.6848\n",
            "[0/25][356/782] Loss_D: 0.2984 Loss_G: 8.0475\n",
            "[0/25][357/782] Loss_D: 0.1643 Loss_G: 6.6354\n",
            "[0/25][358/782] Loss_D: 0.0538 Loss_G: 5.4364\n",
            "[0/25][359/782] Loss_D: 0.2038 Loss_G: 7.4179\n",
            "[0/25][360/782] Loss_D: 0.1581 Loss_G: 6.6340\n",
            "[0/25][361/782] Loss_D: 0.0975 Loss_G: 5.4165\n",
            "[0/25][362/782] Loss_D: 0.1092 Loss_G: 7.3363\n",
            "[0/25][363/782] Loss_D: 0.0776 Loss_G: 6.6202\n",
            "[0/25][364/782] Loss_D: 0.0841 Loss_G: 5.5190\n",
            "[0/25][365/782] Loss_D: 0.1361 Loss_G: 8.1864\n",
            "[0/25][366/782] Loss_D: 0.2260 Loss_G: 6.2117\n",
            "[0/25][367/782] Loss_D: 0.0715 Loss_G: 5.6477\n",
            "[0/25][368/782] Loss_D: 0.1894 Loss_G: 7.5787\n",
            "[0/25][369/782] Loss_D: 0.0813 Loss_G: 6.9495\n",
            "[0/25][370/782] Loss_D: 0.2778 Loss_G: 5.2531\n",
            "[0/25][371/782] Loss_D: 0.3722 Loss_G: 16.3453\n",
            "[0/25][372/782] Loss_D: 0.5291 Loss_G: 14.7901\n",
            "[0/25][373/782] Loss_D: 0.2695 Loss_G: 10.3245\n",
            "[0/25][374/782] Loss_D: 0.0093 Loss_G: 5.9121\n",
            "[0/25][375/782] Loss_D: 0.2179 Loss_G: 7.6716\n",
            "[0/25][376/782] Loss_D: 0.0478 Loss_G: 7.8295\n",
            "[0/25][377/782] Loss_D: 0.0677 Loss_G: 6.8808\n",
            "[0/25][378/782] Loss_D: 0.1433 Loss_G: 8.7731\n",
            "[0/25][379/782] Loss_D: 0.0956 Loss_G: 8.6138\n",
            "[0/25][380/782] Loss_D: 0.3073 Loss_G: 7.5347\n",
            "[0/25][381/782] Loss_D: 0.5913 Loss_G: 14.5703\n",
            "[0/25][382/782] Loss_D: 0.2284 Loss_G: 13.4280\n",
            "[0/25][383/782] Loss_D: 0.1936 Loss_G: 8.4127\n",
            "[0/25][384/782] Loss_D: 0.2663 Loss_G: 6.3913\n",
            "[0/25][385/782] Loss_D: 0.5218 Loss_G: 17.8820\n",
            "[0/25][386/782] Loss_D: 0.9227 Loss_G: 17.3841\n",
            "[0/25][387/782] Loss_D: 0.1003 Loss_G: 14.3405\n",
            "[0/25][388/782] Loss_D: 0.1247 Loss_G: 10.2119\n",
            "[0/25][389/782] Loss_D: 0.0326 Loss_G: 5.8322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t3KSdKQtW66z"
      }
    }
  ]
}